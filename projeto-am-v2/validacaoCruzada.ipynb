{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import utils\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = utils.get_base_de_dados()\r\n",
    "y = dados['SEQUENCE NAME']\r\n",
    "x = dados.drop('SEQUENCE NAME', axis=1)\r\n",
    "x_treino, x_teste , y_treino, y_teste = train_test_split(x,y,test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelos (a, b):\r\n",
    "    from classificador_bayesiano import ClassificadorBayesiano\r\n",
    "    from sklearn.neighbors import KNeighborsClassifier\r\n",
    "    from sklearn.linear_model import LogisticRegression\r\n",
    "    from sklearn.tree import DecisionTreeClassifier\r\n",
    "    from sklearn.ensemble import VotingClassifier\r\n",
    "    \r\n",
    "\r\n",
    "    x = a\r\n",
    "    y = b\r\n",
    "\r\n",
    "    kfold = KFold(n_splits=10)\r\n",
    "\r\n",
    "    bayes_clf = ClassificadorBayesiano()\r\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=5)\r\n",
    "    reg_log_clf = LogisticRegression(max_iter=200)\r\n",
    "    arv_dec_clf = DecisionTreeClassifier()\r\n",
    "    ensemble = VotingClassifier(estimators=[('bayes_clf', bayes_clf), ('knn_clf', knn_clf),('reg_log_clf', reg_log_clf), ('arv_dec_clf', arv_dec_clf)], voting='hard')\r\n",
    "\r\n",
    "\r\n",
    "    bayes_clf_cv = cross_val_score(bayes_clf, x, y, cv = kfold).mean()\r\n",
    "    knn_clf_cv = cross_val_score(knn_clf, x, y, cv = kfold).mean()\r\n",
    "    reg_log_clf_cv = cross_val_score(reg_log_clf, x, y, cv = kfold).mean()\r\n",
    "    arv_dec_clf_cv = cross_val_score(arv_dec_clf, x, y, cv = kfold).mean()\r\n",
    "    ensemble_cv = cross_val_score(ensemble, x, y, cv = kfold).mean()\r\n",
    "\r\n",
    "    dic_modelos = {'Bayes':bayes_clf_cv, 'KNN':knn_clf_cv, 'Regressao':reg_log_clf_cv, 'Arvore':arv_dec_clf_cv, 'Ensemble':ensemble_cv}\r\n",
    "    melhor_modelo = max(dic_modelos, key=dic_modelos.get)\r\n",
    "\r\n",
    "    print('Bayes', bayes_clf_cv, '\\nKNN', knn_clf_cv, '\\nRegressao', reg_log_clf_cv, '\\nArvore', arv_dec_clf_cv, '\\nEnsemble', ensemble_cv)\r\n",
    "    print('\\nO melhor modelo é:', melhor_modelo)\r\n",
    "    print('Com valor de:', dic_modelos[melhor_modelo])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes 0.27095048068202426 \n",
      "KNN 0.5410575004534737 \n",
      "Regressao 0.5322555777253765 \n",
      "Arvore 0.47769363323054603 \n",
      "Ensemble 0.5410302920370035\n",
      "\n",
      "O melhor modelo é: KNN\n",
      "Com valor de: 0.5410575004534737\n"
     ]
    }
   ],
   "source": [
    "modelos(x, y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}